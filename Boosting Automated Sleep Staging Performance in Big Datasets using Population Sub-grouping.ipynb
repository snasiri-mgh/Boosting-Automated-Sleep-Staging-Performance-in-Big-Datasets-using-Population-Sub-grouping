{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://drive.google.com/drive/folders/1vokakruDSSCE33GKZlis25OTSSw95mAD?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import fftconvolve\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import gc as garbageCollector\n",
    "from multiprocessing import Pool\n",
    "\n",
    "'''Script to extract preprocessed signals and save to temporary numpy memap files to be used for model training'''\n",
    "\n",
    "########################################################################################################################\n",
    "# Function to compute consistent train/validation/test split by a constant random seed\n",
    "\n",
    "def computeTrainValidationTestRecords(dataPath, foldName='Default'):\n",
    "    print('////////////////////')\n",
    "    print('Computing train/validation/test data split, Test results are:')\n",
    "\n",
    "    # Define consistent mappings for the index from training data to the actual returned value to ensure shuffling\n",
    "    r = random.Random()\n",
    "    r.seed(29)\n",
    "    requestRecordMap = list(range(994))\n",
    "    r.shuffle(requestRecordMap)\n",
    "\n",
    "    #########\n",
    "    if foldName == 'Default':\n",
    "        testIndices = requestRecordMap[0:200]\n",
    "        validationIndices = requestRecordMap[894::]\n",
    "        trainIndices = requestRecordMap[200:894]\n",
    "    elif foldName == 'Auxiliary1':\n",
    "        testIndices = requestRecordMap[0:100]\n",
    "        validationIndices = requestRecordMap[100:200]\n",
    "        trainIndices = requestRecordMap[200::]\n",
    "    elif foldName == 'Auxiliary2':\n",
    "        testIndices = requestRecordMap[0:100]\n",
    "        validationIndices = requestRecordMap[300:400]\n",
    "        trainIndices = requestRecordMap[100:300]\n",
    "        trainIndices.extend(requestRecordMap[400::])\n",
    "    elif foldName == 'Auxiliary3':\n",
    "        testIndices = requestRecordMap[0:100]\n",
    "        validationIndices = requestRecordMap[600:700]\n",
    "        trainIndices = requestRecordMap[100:600]\n",
    "        trainIndices.extend(requestRecordMap[700::])\n",
    "    elif foldName == 'Auxiliary4':\n",
    "        testIndices = requestRecordMap[0:100]\n",
    "        validationIndices = requestRecordMap[894::]\n",
    "        trainIndices = requestRecordMap[100:894]\n",
    "\n",
    "    #########\n",
    "\n",
    "    recordList = list(filter(lambda x: os.path.isdir(dataPath + x),\n",
    "                        os.listdir(dataPath)))\n",
    "    trainRecordList = [recordList[ind] for ind in trainIndices]\n",
    "    validationRecordList = [recordList[ind] for ind in validationIndices]\n",
    "    testRecordList = [recordList[ind] for ind in testIndices]\n",
    "\n",
    "    # Small test to make sure the sets are non overlapping and use all of the data\n",
    "    areUnique = len(list(set(trainRecordList).intersection(set(validationRecordList).intersection(testRecordList)))) == 0\n",
    "    isComplete = len(list(set(trainRecordList).union(set(validationRecordList).union(testRecordList)).union(set(recordList)))) == len(recordList)\n",
    "\n",
    "    print('Uniqueness Test: ' + str(areUnique) + '  Completeness Test: ' + str(isComplete))\n",
    "    print('////////////////////\\n')\n",
    "\n",
    "    return trainRecordList, validationRecordList, testRecordList\n",
    "\n",
    "def computeUnseenRecords(dataPath):\n",
    "    recordList = filter(lambda x: os.path.isdir(dataPath + x),\n",
    "                        os.listdir(dataPath))\n",
    "\n",
    "    return recordList\n",
    "\n",
    "\n",
    "########################################################################################################################\n",
    "# Load signals or annotations from a specific file in the source files\n",
    "\n",
    "# Convenience function to load signals\n",
    "def loadSignals(recordName, dataPath):\n",
    "    signals = loadmat(dataPath + recordName + '/' + recordName + '.mat')\n",
    "    signals = signals['val']\n",
    "    garbageCollector.collect()\n",
    "\n",
    "    return signals\n",
    "\n",
    "# Convenience function to load annotations\n",
    "def loadAnnotations(recordName, arousalAnnotationPath, apneaHypopneaAnnotationPath, sleepWakeAnnotationPath):\n",
    "\n",
    "    arousalAnnotations = loadmat(arousalAnnotationPath + recordName + '-arousal.mat')['data']['arousals'][0][0]\n",
    "    arousalAnnotations = np.squeeze(arousalAnnotations.astype(np.int32))\n",
    "    garbageCollector.collect()\n",
    "\n",
    "    fp = np.memmap(apneaHypopneaAnnotationPath + 'apneaHypopneaAnnotation_' + str(recordName) + '.dat', dtype='int32', mode='r')\n",
    "    apneaHypopneaAnnotations = np.zeros(shape=fp.shape)\n",
    "    apneaHypopneaAnnotations[:] = fp[:]\n",
    "\n",
    "    fp = np.memmap(sleepWakeAnnotationPath + 'sleepWakeAnnotation_' + str(recordName) + '.dat', dtype='int32',  mode='r')\n",
    "    sleepStageAnnotations = np.zeros(shape=fp.shape)\n",
    "    sleepStageAnnotations[:] = fp[:]\n",
    "\n",
    "    return arousalAnnotations, apneaHypopneaAnnotations, sleepStageAnnotations\n",
    "\n",
    "########################################################################################################################\n",
    "# Load signals or annotations from a specific file in the source files\n",
    "\n",
    "def extractWholeRecord(recordName,\n",
    "                       dataPath,\n",
    "                       arousalAnnotationPath,\n",
    "                       apneaHypopneaAnnotationPath,\n",
    "                       sleepWakeAnnotationPath,\n",
    "                       extractAnnotations=True):\n",
    "    # Keep all EEG channels\n",
    "    keepChannels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "    if extractAnnotations:\n",
    "        arousalAnnotations, apneaHypopneaAnnotations, sleepStageAnnotations = loadAnnotations(recordName, arousalAnnotationPath, apneaHypopneaAnnotationPath, sleepWakeAnnotationPath)\n",
    "\n",
    "    signals = loadSignals(recordName, dataPath)\n",
    "    signals = np.transpose(signals).astype(np.float64)\n",
    "\n",
    "    # Apply antialiasing FIR filter to each channel and downsample to 50Hz\n",
    "    filtCoeff = np.array([0.00637849379422531, 0.00543091599801427, -0.00255136650039784, -0.0123109503066702,\n",
    "                          -0.0137267267561505, -0.000943230632358082, 0.0191919895027550, 0.0287148886882440,\n",
    "                          0.0123598773891149, -0.0256928886371578, -0.0570987715759348, -0.0446385294777459,\n",
    "                          0.0303553522906817, 0.148402006671856, 0.257171285176269, 0.301282456398562,\n",
    "                          0.257171285176269, 0.148402006671856, 0.0303553522906817, -0.0446385294777459,\n",
    "                          -0.0570987715759348, -0.0256928886371578, 0.0123598773891149, 0.0287148886882440,\n",
    "                          0.0191919895027550, -0.000943230632358082, -0.0137267267561505, -0.0123109503066702,\n",
    "                          -0.00255136650039784, 0.00543091599801427, 0.00637849379422531])\n",
    "\n",
    "    for n in range(signals.shape[1]):\n",
    "        signals[::, n] = np.convolve(signals[::, n], filtCoeff, mode='same')\n",
    "\n",
    "    signals = signals[0::4, keepChannels]\n",
    "    if extractAnnotations:\n",
    "        arousalAnnotations = arousalAnnotations[0::4]\n",
    "        apneaHypopneaAnnotation = apneaHypopneaAnnotations[0::4]\n",
    "        sleepStageAnnotation = sleepStageAnnotations[0::4]\n",
    "\n",
    "    garbageCollector.collect()\n",
    "\n",
    "    # Scale SaO2 to sit between -0.5 and 0.5, a good range for input to neural network\n",
    "    signals[::, 11] += -32768.0\n",
    "    signals[::, 11] /= 65535.0\n",
    "    signals[::, 11] -= 0.5\n",
    "\n",
    "    # Normalize all the other channels by removing the mean and the rms in an 18 minute rolling window, using fftconvolve for computational efficiency\n",
    "    # 18 minute window is used because because baseline breathing is established in 2 minute window according to AASM standards.\n",
    "    # Normalizing over 18 minutes ensure a 90% overlap between the beginning and end of the baseline window\n",
    "    kernel_size = (50*18*60)+1\n",
    "\n",
    "    # Remove DC bias and scale for FFT convolution\n",
    "    center = np.mean(signals, axis=0)\n",
    "    scale = np.std(signals, axis=0)\n",
    "    scale[scale == 0] = 1.0\n",
    "    signals = (signals - center) / scale\n",
    "\n",
    "    # Compute and remove moving average with FFT convolution\n",
    "    center = np.zeros(signals.shape)\n",
    "    for n in range(signals.shape[1]):\n",
    "        center[::, n] = fftconvolve(signals[::, n], np.ones(shape=(kernel_size,))/kernel_size, mode='same')\n",
    "\n",
    "    # Exclude SAO2\n",
    "    center[::, 11] = 0.0\n",
    "    center[np.isnan(center) | np.isinf(center)] = 0.0\n",
    "    signals = signals - center\n",
    "\n",
    "    # Compute and remove the rms with FFT convolution of squared signal\n",
    "    scale = np.ones(signals.shape)\n",
    "    for n in range(signals.shape[1]):\n",
    "        temp = fftconvolve(np.square(signals[::, n]), np.ones(shape=(kernel_size,))/kernel_size, mode='same')\n",
    "\n",
    "        # Deal with negative values (mathematically, it should never be negative, but fft artifacts can cause this)\n",
    "        temp[temp < 0] = 0.0\n",
    "\n",
    "        # Deal with invalid values\n",
    "        invalidIndices = np.isnan(temp) | np.isinf(temp)\n",
    "        temp[invalidIndices] = 0.0\n",
    "        maxTemp = np.max(temp)\n",
    "        temp[invalidIndices] = maxTemp\n",
    "\n",
    "        # Finish rms calculation\n",
    "        scale[::, n] = np.sqrt(temp)\n",
    "\n",
    "    # Exclude SAO2\n",
    "    scale[::, 11] = 1.0\n",
    "\n",
    "    scale[(scale == 0) | np.isinf(scale) | np.isnan(scale)] = 1.0  # To correct for record 12 that has a zero amplitude chest signal\n",
    "    signals = signals / scale\n",
    "\n",
    "    garbageCollector.collect()\n",
    "\n",
    "    # Convert to 32 bits\n",
    "    signals = signals.astype(np.float32)\n",
    "\n",
    "    if extractAnnotations:\n",
    "        arousalAnnotations = np.expand_dims(arousalAnnotations, axis=1).astype(np.float32)\n",
    "        apneaHypopneaAnnotation = np.expand_dims(apneaHypopneaAnnotation, axis=1).astype(np.float32)\n",
    "        sleepStageAnnotation = np.expand_dims(sleepStageAnnotation, axis=1).astype(np.float32)\n",
    "        return np.concatenate([signals, arousalAnnotations, apneaHypopneaAnnotation, sleepStageAnnotation], axis=1)\n",
    "    else:\n",
    "        return signals\n",
    "\n",
    "########################################################################################################################\n",
    "# Functions to extract all three datasets\n",
    "\n",
    "def extractBatchedData(datasetName,\n",
    "                       dataPath,\n",
    "                       savePath,\n",
    "                       foldName,\n",
    "                       dataLimitInHours=7,\n",
    "                       useMultiprocessing=False):\n",
    "\n",
    "    # Get list of records and divide into train, validation and testing based on pre defined randomly divided folds\n",
    "    trainRecordList, validationRecordList, testRecordList = computeTrainValidationTestRecords(dataPath, foldName=foldName)\n",
    "    testRecordList = None\n",
    "\n",
    "    if datasetName == 'Training':\n",
    "        recordList = trainRecordList\n",
    "        validationRecordList = None\n",
    "    elif datasetName == 'Validation':\n",
    "        recordList = validationRecordList\n",
    "        trainRecordList = None\n",
    "    else:\n",
    "        raise Exception('Invalid data set name for batched data extraction!')\n",
    "\n",
    "    # Set up\n",
    "    numRecordsPerBatch = 11\n",
    "    batchCount = 0\n",
    "    sampleDataLimit = dataLimitInHours*3600*50\n",
    "\n",
    "    extractDatasetFilePath = savePath + datasetName + '_'\n",
    "\n",
    "    if useMultiprocessing:\n",
    "        pool = Pool(processes=numRecordsPerBatch)\n",
    "\n",
    "    fp = np.memmap(extractDatasetFilePath + 'data.dat', dtype='float32', mode='w+', shape=(len(recordList), sampleDataLimit, 15))\n",
    "\n",
    "    # Extract data in batches of numRecordsPerBatch\n",
    "    for n in range(0, len(recordList), numRecordsPerBatch):\n",
    "\n",
    "        print('Extracting Batch: ' + str(batchCount))\n",
    "\n",
    "        # Compute upper limit for this batch of records to extract\n",
    "        limit = n+numRecordsPerBatch\n",
    "        if (n+numRecordsPerBatch) > len(recordList):\n",
    "            limit = len(recordList)\n",
    "\n",
    "        # Process and extract batch of records\n",
    "        if useMultiprocessing:\n",
    "            data = pool.map(extractWholeRecord, recordList[n:limit])\n",
    "        else:\n",
    "            data = list(map(extractWholeRecord, recordList[n:limit]))\n",
    "\n",
    "        # Enforce dataLimitInHours hour length with chopping / zero padding for memory usage stability and effficiency in cuDNN\n",
    "        for n in range(len(data)):\n",
    "            originalLength = data[n].shape[0]/(3600*50)\n",
    "            if data[n].shape[0] < sampleDataLimit:\n",
    "                # Zero Pad\n",
    "                neededLength = sampleDataLimit - data[n].shape[0]\n",
    "                extension = np.zeros(shape=(neededLength, data[n].shape[1]))\n",
    "                extension[::, -3::] = -1.0\n",
    "                data[n] = np.concatenate([data[n], extension], axis=0)\n",
    "\n",
    "            elif data[n].shape[0] > sampleDataLimit:\n",
    "                # Chop\n",
    "                data[n] = data[n][0:sampleDataLimit, ::]\n",
    "\n",
    "            print('Original Length: ' + str(originalLength) + '  New Length: ' + str(data[n].shape[0]/(3600*50)))\n",
    "\n",
    "        garbageCollector.collect()\n",
    "\n",
    "        print('Saving Batch: ' + str(batchCount))\n",
    "\n",
    "        # Save batch of extracted records to extracted data path\n",
    "        for m in range(len(data)):\n",
    "            fp[(batchCount*numRecordsPerBatch)+m, ::, ::] = data[m][::, ::]\n",
    "            assert isinstance(fp, np.memmap)\n",
    "            fp.flush()\n",
    "\n",
    "        garbageCollector.collect()\n",
    "\n",
    "        batchCount += 1\n",
    "\n",
    "    del fp\n",
    "\n",
    "# Extract training and validation sets from backing store, YOU NEED TO COMPLETE THE REQURIED INPUTS BASED ON YOUR ENVIRONMENT\n",
    "extractBatchedData(useMultiprocessing=True, datasetName='Training', foldName='Auxiliary4', dataPath, savePath)\n",
    "extractBatchedData(useMultiprocessing=True, datasetName='Validation', foldName='Auxiliary4', dataPath, savePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the covaraince matrices for different records \n",
    "\n",
    "we do this in an unsupervise setting (no labeled needed in this stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we used pyriemann library which is a machine learning library based on scikit-learn API\n",
    "import sys\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.clustering import Kmeans\n",
    "from pyriemann.utils.distance import pairwise_distance\n",
    "from sklearn.manifold import TSNE\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.tangentspace import tangent_space\n",
    "\n",
    "\n",
    "## Note: if covaraince matrix is not SPD, it means that at least one of variables \n",
    "## can be expressed as a linear combinanttion of the others. \n",
    "\n",
    "recordList = sys.argv[1:]\n",
    "\n",
    "COV = []\n",
    "\n",
    "for record in recordList:\n",
    "    \n",
    "    processedSignal = extractWholeRecord(recordName=str(record),\n",
    "                                             dataPath='./',\n",
    "                                             dataInDirectory=False)\n",
    "    processedSignal.apply(zscore)\n",
    "    xtrain =  processedSignal.values\n",
    "    \n",
    "    # In order to create symmetric positive definite matrices we consider sigma (shrinkage model)\n",
    "    \n",
    "    COV = np.append(COV, [np.cov(xtrain.T) + sigma * np.identity(processedSignal.shape[1])], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering on a Riemannian manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans= Kmeans(10).fit(COV)\n",
    "clusters = kmeans.predict(COV)\n",
    "\n",
    "\n",
    "# Plotting the clustering results\n",
    "\n",
    "distmatrix = pairwise_distance(COV, metric='riemann')\n",
    "X_embedded = TSNE(n_components=2, metric='precomputed').fit_transform(distmatrix)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), facecolor='white')\n",
    "\n",
    "target_ids = np.unique(clusters)\n",
    "colors = 'r', 'g', 'b', 'c', 'm'\n",
    "for i, c, label in zip(target_ids, colors, target_ids):\n",
    "    ax.scatter(X_embedded [clusters == i , 0], X_embedded[clusters == i, 1], c=c, label=label)\n",
    "\n",
    "ax.set_xlabel(r'$\\varphi_1$', fontsize=16)\n",
    "ax.set_ylabel(r'$\\varphi_2$', fontsize=16)\n",
    "ax.set_title('Sub-grouping the population using a Riemannian Distance', fontsize=16)\n",
    "\n",
    "ax.grid(False)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a CNN model for each cluster "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping covariance matrices to tangent space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tangent space project TransformerMixin.\n",
    "\n",
    "Tangent space projection map a set of covariance matrices to their tangent space. The Tangent space projection can be seen as a kernel operation. After projection, each matrix is represented as a vector of size 𝑁(𝑁+1)/2 where N is the dimension of the covariance matrices.\n",
    "\n",
    "Tangent space projection is useful to convert covariance matrices in euclidean vectors while conserving the inner structure of the manifold. After projection, standard processing and vector-based classification can be applied.\n",
    "\n",
    "Tangent space projection is a local approximation of the manifold. it takes one parameter, the reference point, that is usually estimated using the geometric mean of the covariance matrices set you project. if the function fit is not called, the identity matrix will be used as reference point. This can lead to serious degradation of performances. The approximation will be bigger if the matrices in the set are scattered in the manifold, and lower if they are grouped in a small region of the manifold.\n",
    "\n",
    "After projection, it is possible to go back in the manifold using the inverse transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: X : ndarray, shape (n_trials, n_channels, n_channels)\n",
    "# Output: Y: ts : ndarray, shape (n_trials, n_ts): the tangent space projection of the matrices.\n",
    "\n",
    "from pyriemann.utils.mean import mean_ale\n",
    "\n",
    "Features = []\n",
    "i = 0 \n",
    "for record in recordList:\n",
    "    \n",
    "    \n",
    "    covmats = COV [i] # Covariance matrices set, Ntrials X Nchannels X Nchannels\n",
    "    i = i + 1\n",
    "    Cref = mean_ale(covmats, tol=1e-06, maxiter=50, sample_weight=None) # the mean covariance matrix\n",
    "    clf = TangentSpace.tangent_space(covmats, Cref)\n",
    "    \n",
    "    # In order to create symmetric positive definite matrices we consider sigma (shrinkage model)\n",
    "\n",
    "    Features = np.append(Features, clf, axis=0) # Data are in the tangent space which is a locally Euclidean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN training\n",
    "The model is similar to the model is used in \"Supervised and unsupervised machine learning for automated scoring of sleep-wake and cataplexy in a mouse model of narcolepsy\", but we used 5-stage classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All algorithms implemented in the mentioned paper can be made available upon request.\n",
    "Therefore, we have released only trained models on our authorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import multiprocessing as mltp\n",
    "import gc as garbageCollector\n",
    "from random import Random\n",
    "\n",
    "########################################################################################################################\n",
    "#\n",
    "\n",
    "# load X_tr and Y_tr (corresponding to each record and 30-sec)\n",
    "# train a CNN model \n",
    "# compute confusion matrix \n",
    "\n",
    "K = 5 \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import gc as garbageCollector\n",
    "\n",
    "from ModelDefinition import Sleep_model_MultiTarget\n",
    "\n",
    "from TrainingDataManager import asyncTrainingDataLoader\n",
    "from ValidationDataManager import asyncValidationDataLoader, evalValidationAccuracy\n",
    "\n",
    "import timeit\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Settings\n",
    "\n",
    "# Control parameters\n",
    "gpuSetting = 1     # Which GPU to use\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpuSetting)\n",
    "\n",
    "foldName = 'Auxiliary1'\n",
    "\n",
    "numChannels = 12\n",
    "\n",
    "########################################################################################################################\n",
    "# Training and validation data asynchronous managers\n",
    "\n",
    "extractedDataPath = \"UPDATE BASED ON YOUR ENVIRONMENT\"\n",
    "trainingData = asyncTrainingDataLoader(extractedDataPath=extractedDataPath, reductionFactor=50, numChannels=numChannels)\n",
    "validationData = asyncValidationDataLoader(extractedDataPath=extractedDataPath, numRecords=100, reductionFactor=50, numChannels=numChannels)\n",
    "garbageCollector.collect()\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Trains the model\n",
    "def trainModel(model):\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    arousalCriterion = torch.nn.CrossEntropyLoss(ignore_index=-1, reduce=True).cuda()\n",
    "    apneaCriterion = torch.nn.CrossEntropyLoss(ignore_index=-1, reduce=True).cuda()\n",
    "    wakeCriterion = torch.nn.CrossEntropyLoss(ignore_index=-1, reduce=True).cuda()\n",
    "\n",
    "    bestArousalAUC = 0.5\n",
    "    bestArousalAP = 0.0\n",
    "\n",
    "    bestApneaAUC = 0.5\n",
    "    bestApneaAP = 0.0\n",
    "\n",
    "    bestWakeAUC = 0.5\n",
    "    bestWakeAP = 0.0\n",
    "\n",
    "    i_epoch = 0\n",
    "\n",
    "    numBatchesPerEpoch = 100\n",
    "\n",
    "    while True:\n",
    "        # Put in train mode\n",
    "        trainingStartTime = timeit.default_timer()\n",
    "\n",
    "        model.train()\n",
    "        runningLoss = 0.0\n",
    "\n",
    "        for n in range(numBatchesPerEpoch):\n",
    "\n",
    "            # Fetch pre-loaded batch from asynchronous data loader\n",
    "            batchFeats, batchArousalTargs, batchApneaTargs, batchWakeTargs = trainingData.getNextItem()\n",
    "\n",
    "            # Send batch to GPU\n",
    "            batchFeats = batchFeats.cuda()\n",
    "            batchArousalTargs = batchArousalTargs.cuda()\n",
    "            batchApneaTargs = batchApneaTargs.cuda()\n",
    "            batchWakeTargs = batchWakeTargs.cuda()\n",
    "\n",
    "            # Compute the network outputs on the batch\n",
    "            arousalOutputs, apneaHypopneaOutputs, sleepStageOutputs = model(batchFeats)\n",
    "\n",
    "            # Compute the losses\n",
    "            arousalOutputs = arousalOutputs.permute(0, 2, 1).contiguous().view(-1, 2)\n",
    "            apneaHypopneaOutputs = apneaHypopneaOutputs.permute(0, 2, 1).contiguous().view(-1, 2)\n",
    "            sleepStageOutputs = sleepStageOutputs.permute(0, 2, 1).contiguous().view(-1, 2)\n",
    "\n",
    "            arousalLoss = arousalCriterion(arousalOutputs, batchArousalTargs)\n",
    "            apneaHypopneaLoss = apneaCriterion(apneaHypopneaOutputs, batchApneaTargs)\n",
    "            sleepStageLoss = wakeCriterion(sleepStageOutputs, batchWakeTargs)\n",
    "\n",
    "            loss = ((2*arousalLoss) + apneaHypopneaLoss + sleepStageLoss) / 4.0\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform one optimization step\n",
    "            currentBatchLoss = loss.data.cpu().numpy()\n",
    "            runningLoss += currentBatchLoss\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        i_epoch += 1\n",
    "\n",
    "        trainingEndTime = timeit.default_timer()\n",
    "\n",
    "        # Get validation accuracy\n",
    "        arousalAUC, arousalAP, apneaAUC, apneaAP, wakeAUC, wakeAP = evalValidationAccuracy(model, validationData=validationData)\n",
    "\n",
    "        validationEndTime = timeit.default_timer()\n",
    "\n",
    "        print('////////////////////')\n",
    "        print('Epoch Number: ' + str(i_epoch) + '  Training Time: ' + str(trainingEndTime - trainingStartTime) + '  Validation Time: ' + str(validationEndTime - trainingEndTime))\n",
    "        print('Average Training Loss: ' + str(runningLoss / float(numBatchesPerEpoch)))\n",
    "       \n",
    "\n",
    "        f = open('./Models/Auxiliary1' + '/checkpointModel_' + str(i_epoch) + '.pkl', 'wb')\n",
    "        torch.save(model, f)\n",
    "        f.close()\n",
    "\n",
    "########################################################################################################################\n",
    "# Create and train model\n",
    "\n",
    "# Create new model and optimizer\n",
    "model = Sleep_model_MultiTarget(numSignals=numChannels)\n",
    "model.cuda()\n",
    "model.train()\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = trainModel(model=model)\n",
    "\n",
    "for k in range(0,K):\n",
    "    \n",
    "    print (\"%%%%%%%%%%%%%%%%%%%%%%%%% working on cluster \" + str(k))\n",
    "    \n",
    "    Cluster = RecordName [clusters_total == k]\n",
    "    \n",
    "    TrainFold = pd.unique(fold [fold.subset == 'train'].id)\n",
    "    TestFold = pd.unique(fold [fold.subset == 'test'].id)\n",
    "    VldFold = pd.unique(fold [fold.subset == 'cv'].id)\n",
    "\n",
    "    ########## Across studies\n",
    "    TrainFiles = list(set(TrainFold).intersection(Cluster))\n",
    "    TestFiles = list(set(TestFold).intersection(Cluster))\n",
    "    VldFiles = list(set(VldFold).intersection(Cluster))\n",
    "    \n",
    "    N_files_train.append(len(TrainFiles)) \n",
    "    N_files_test.append(len(TrainFiles)) \n",
    "    N_files_vld.append(len(TrainFiles)) \n",
    "    \n",
    "    Xtr = np.empty((0, Features.shape[1]))\n",
    "    Ytr = np.empty((0))\n",
    "    \n",
    "    #create training dataset for the cluster\n",
    "    for i in TrainFiles:\n",
    "    \n",
    "        print (i)\n",
    "        X = Features[Subjects.subject == i]\n",
    "        X.apply(zscore)\n",
    "        Xtr = np.concatenate((Xtr, X.values), axis=0)\n",
    "        Y = Labels[Subjects['subject']== i]\n",
    "        label_tr = Y.values.reshape(len(Y))\n",
    "        Ytr = np.concatenate((Ytr, label_tr))\n",
    "    \n",
    "    # create test dataset for the cluster\n",
    "    Xte = np.empty((0, Features.shape[1]))\n",
    "    Yte = np.empty((0))\n",
    "    print(\"wokring on test records\")\n",
    "    for i in TestFiles:\n",
    "        print (i)\n",
    "        X = Features[Subjects.subject == i]\n",
    "        X.apply(zscore)\n",
    "        Xte = np.concatenate((Xte, X.values), axis=0)\n",
    "        Y = Labels[Subjects.subject == i]\n",
    "        label_te = Y.values.reshape(len(Y))\n",
    "        Yte = np.concatenate((Yte, label_te))\n",
    "\n",
    "    # create Validation dataset for the cluster\n",
    "    Xvld = np.empty((0, Features.shape[1]))\n",
    "    Yvld = np.empty((0))\n",
    "    print(\"wokring on test records\")\n",
    "    for i in VldFiles:\n",
    "        print (i)\n",
    "        X = Features[Subjects.subject == i]\n",
    "        X.apply(zscore)\n",
    "        Xvld = np.concatenate((Xvld, X.values), axis=0)\n",
    "        Y = Labels[Subjects.subject == i]\n",
    "        label_vld = Y.values.reshape(len(Y))\n",
    "        Yvld = np.concatenate((Yvld, label_vld))\n",
    "\n",
    "    file = open('TestDataset_features_cluster' + str(k) + '.pkl','wb')\n",
    "    pickle.dump(Xte, file)\n",
    "    pickle.dump(Yte, file)\n",
    "\n",
    "    file = open('VldDataset_features_cluster' + str(k) + '.pkl','wb')\n",
    "    pickle.dump(Xvld, file)\n",
    "    pickle.dump(Yvld, file)\n",
    "\n",
    "\n",
    "    #Randomly Shuffle the data - 5 class classification\n",
    "    s_tr = np.arange(X_tr_resampled.shape[0])\n",
    "    xtrain =  X_tr_resampled[s_tr]\n",
    "    ytrain =  Y_tr_resampled[s_tr]\n",
    "\n",
    "    s_vld = np.arange(Yvld.shape[0])\n",
    "    xvald =  Xvld[s_vld]\n",
    "    yvald =  Yvld[s_vld]\n",
    "\n",
    "    s_te = np.arange(Yte.shape[0])\n",
    "    xtest =  Xte[s_te]\n",
    "    ytest =  Yte[s_te]\n",
    "    \n",
    "    ## Multi-Stage sleep staging (5-class classification) \n",
    "    \n",
    "    ytr = np.empty_like (ytrain)\n",
    "    ytr[:] = ytrain\n",
    "    \n",
    "    yvld = np.empty_like (yvald)\n",
    "    yvld[:] = yvald\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    file = open('TrainedClassifier_Cluster' + str(k) +'.pkl','wb')\n",
    "    pickle.dump(mlp, file)\n",
    "    predictions = mlp.predict(xtest1)\n",
    "    report = classification_report(ytest1, predictions)\n",
    "    print (\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% cluster\" + str(k))\n",
    "    print (report)\n",
    "    print (\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    from sklearn import metrics\n",
    "    import seaborn as sns\n",
    "    LABELS = np.unique(ytest)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    cm = confusion_matrix(ytest1, predictions)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm, xticklabels=LABELS, yticklabels = LABELS, annot=True, fmt=\"f\");\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('ConfusionMatrix_Cluster' + str(k) +'.pdf',bbox_inches = 'tight',\n",
    "        pad_inches = 0)\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load X_tr and Y_tr (corresponding to each record and 30-sec)\n",
    "# train a CNN model \n",
    "# compute confusion matrix \n",
    "\n",
    "K = 5 \n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import gc as garbageCollector\n",
    "\n",
    "from ModelDefinition import Sleep_model_MultiTarget\n",
    "\n",
    "from TrainingDataManager import asyncTrainingDataLoader\n",
    "from ValidationDataManager import asyncValidationDataLoader, evalValidationAccuracy\n",
    "\n",
    "import timeit\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# training CNN for each cluster: each cluster has training, validation, and test subjects\n",
    "\n",
    "for k in range(0,K):\n",
    "    \n",
    "    print (\"%%%%%%%%%%%%%%%%%%%%%%%%% working on cluster \" + str(k))\n",
    "    \n",
    "    Cluster = RecordName [clusters_total == k]\n",
    "    \n",
    "    TrainFold = pd.unique(fold [fold.subset == 'train'].id)\n",
    "    TestFold = pd.unique(fold [fold.subset == 'test'].id)\n",
    "    VldFold = pd.unique(fold [fold.subset == 'cv'].id)\n",
    "\n",
    "    ########## Across studies\n",
    "    TrainFiles = list(set(TrainFold).intersection(Cluster))\n",
    "    TestFiles = list(set(TestFold).intersection(Cluster))\n",
    "    VldFiles = list(set(VldFold).intersection(Cluster))\n",
    "    \n",
    "    N_files_train.append(len(TrainFiles)) \n",
    "    N_files_test.append(len(TrainFiles)) \n",
    "    N_files_vld.append(len(TrainFiles)) \n",
    "    \n",
    "    Xtr = np.empty((0, Features.shape[1]))\n",
    "    Ytr = np.empty((0))\n",
    "    \n",
    "    #create training dataset for the cluster\n",
    "    for i in TrainFiles:\n",
    "    \n",
    "        print (i)\n",
    "        X = Features[Subjects.subject == i]\n",
    "        X.apply(zscore)\n",
    "        Xtr = np.concatenate((Xtr, X.values), axis=0)\n",
    "        Y = Labels[Subjects['subject']== i]\n",
    "        label_tr = Y.values.reshape(len(Y))\n",
    "        Ytr = np.concatenate((Ytr, label_tr))\n",
    "    \n",
    "    # create test dataset for the cluster\n",
    "    Xte = np.empty((0, Features.shape[1]))\n",
    "    Yte = np.empty((0))\n",
    "    print(\"wokring on test records\")\n",
    "    for i in TestFiles:\n",
    "        print (i)\n",
    "        X = Features[Subjects.subject == i]\n",
    "        X.apply(zscore)\n",
    "        Xte = np.concatenate((Xte, X.values), axis=0)\n",
    "        Y = Labels[Subjects.subject == i]\n",
    "        label_te = Y.values.reshape(len(Y))\n",
    "        Yte = np.concatenate((Yte, label_te))\n",
    "\n",
    "    # create Validation dataset for the cluster\n",
    "    Xvld = np.empty((0, Features.shape[1]))\n",
    "    Yvld = np.empty((0))\n",
    "    print(\"wokring on test records\")\n",
    "    for i in VldFiles:\n",
    "        print (i)\n",
    "        X = Features[Subjects.subject == i]\n",
    "        X.apply(zscore)\n",
    "        Xvld = np.concatenate((Xvld, X.values), axis=0)\n",
    "        Y = Labels[Subjects.subject == i]\n",
    "        label_vld = Y.values.reshape(len(Y))\n",
    "        Yvld = np.concatenate((Yvld, label_vld))\n",
    "\n",
    "    file = open('TestDataset_features_cluster' + str(k) + '.pkl','wb')\n",
    "    pickle.dump(Xte, file)\n",
    "    pickle.dump(Yte, file)\n",
    "\n",
    "    file = open('VldDataset_features_cluster' + str(k) + '.pkl','wb')\n",
    "    pickle.dump(Xvld, file)\n",
    "    pickle.dump(Yvld, file)\n",
    "\n",
    "\n",
    "    #Randomly Shuffle the data - 5 class classification\n",
    "    X_tr_resampled, y_tr_resampled = ADSYN().fit_resample(Xtr, Ytr) # Resampling for training step\n",
    "    s_tr = np.arange(X_tr_resampled.shape[0])\n",
    "    xtrain =  X_tr_resampled[s_tr]\n",
    "    ytrain =  Y_tr_resampled[s_tr]\n",
    "\n",
    "    s_vld = np.arange(Yvld.shape[0])\n",
    "    xvald =  Xvld[s_vld]\n",
    "    yvald =  Yvld[s_vld]\n",
    "\n",
    "    s_te = np.arange(Yte.shape[0])\n",
    "    xtest =  Xte[s_te]\n",
    "    ytest =  Yte[s_te]\n",
    "    \n",
    "    ## Multi-Stage sleep staging (5-class classification) \n",
    "    \n",
    "    ytr = np.empty_like (ytrain)\n",
    "    ytr[:] = ytrain\n",
    "    \n",
    "    yvld = np.empty_like (yvald)\n",
    "    yvld[:] = yvald\n",
    "    \n",
    "    \n",
    "    ### Training 5 CNN models for each cluster\n",
    "\n",
    "    file = open('TrainedClassifier_Cluster' + str(k) +'.pkl','wb')\n",
    "    pickle.dump(mlp, file)\n",
    "    predictions = mlp.predict(xtest1)\n",
    "    report = classification_report(ytest1, predictions)\n",
    "    print (\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% cluster\" + str(k))\n",
    "    print (report)\n",
    "    print (\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    from sklearn import metrics\n",
    "    import seaborn as sns\n",
    "    LABELS = np.unique(ytest)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    cm = confusion_matrix(ytest1, predictions)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm, xticklabels=LABELS, yticklabels = LABELS, annot=True, fmt=\"f\");\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('ConfusionMatrix_Cluster' + str(k) +'.pdf',bbox_inches = 'tight',\n",
    "        pad_inches = 0)\n",
    "    plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
